# backend/routers/chat.py

from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from typing import List
import os
from dotenv import load_dotenv
load_dotenv()
import openai

router = APIRouter(prefix="/api/chat", tags=["chat"])

# OpenAI API í‚¤ ì„¤ì • (í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê¸°)
# ì‹¤ì œ ë°°í¬ ì‹œì—ëŠ” .env íŒŒì¼ì— ë„£ì–´ì•¼ í•©ë‹ˆë‹¤.
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
client = openai.OpenAI(api_key=OPENAI_API_KEY)

# ìš”ì²­ ë°ì´í„° êµ¬ì¡°
class Message(BaseModel):
    role: str # "user" ë˜ëŠ” "assistant"
    content: str

class ChatRequest(BaseModel):
    messages: List[Message]

# ğŸ¤– ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (AIì˜ ì„±ê²© ì„¤ì •)
SYSTEM_PROMPT = """
ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ì „ë¬¸ì ì¸ 'ì•½ì‚¬ AI'ì…ë‹ˆë‹¤. 
ì‚¬ìš©ìê°€ ì¦ìƒì„ ë§í•˜ë©´ ë‹¤ìŒ ì›ì¹™ì— ë”°ë¼ ë‹µë³€í•˜ì„¸ìš”:

1. ì¦ìƒì— ëŒ€í•´ ê³µê°í•˜ê³ , ì˜ˆìƒë˜ëŠ” ì›ì¸ì„ ê°„ë‹¨íˆ ì„¤ëª…í•˜ì„¸ìš”.
2. í•´ë‹¹ ì¦ìƒì— íš¨ê³¼ì ì¸ ì¼ë°˜ì˜ì•½í’ˆ ì„±ë¶„ì´ë‚˜ ì•½ ì´ë¦„ì„ 2~3ê°œ ì¶”ì²œí•˜ì„¸ìš”. (ì˜ˆ: íƒ€ì´ë ˆë†€, ë² ì•„ì œ ë“± í•œêµ­ ì•½ ìœ„ì£¼)
3. ë‹µë³€ ëì—ëŠ” ë°˜ë“œì‹œ "ì •í™•í•œ ì§„ë‹¨ì€ ì˜ì‚¬ë‚˜ ì•½ì‚¬ì™€ ìƒë‹´í•˜ì„¸ìš”."ë¼ëŠ” ì£¼ì˜ì‚¬í•­ì„ ë§ë¶™ì´ì„¸ìš”.
4. ë„ˆë¬´ ì‹¬ê°í•œ ì¦ìƒ(í”¼ê°€ ë‚¨, ì˜ì‹ ë¶ˆëª… ë“±)ì´ë©´ ì¦‰ì‹œ ë³‘ì›ì— ê°€ë¼ê³  ê°•í•˜ê²Œ ê¶Œìœ í•˜ì„¸ìš”.
5. ë§íˆ¬ëŠ” ì •ì¤‘í•˜ê³  ë”°ëœ»í•˜ê²Œ í•˜ì„¸ìš”.
"""

@router.post("")
async def chat_with_ai(request: ChatRequest):
    try:
        # ëŒ€í™” ê¸°ë¡ì— ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì¶”ê°€
        messages = [{"role": "system", "content": SYSTEM_PROMPT}] + [
            {"role": m.role, "content": m.content} for m in request.messages
        ]

        # GPT í˜¸ì¶œ
        response = client.chat.completions.create(
            model="gpt-3.5-turbo", # ë˜ëŠ” "gpt-4"
            messages=messages,
            temperature=0.7,
        )

        ai_reply = response.choices[0].message.content
        return {"reply": ai_reply}

    except Exception as e:
        print(f"Chat Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))